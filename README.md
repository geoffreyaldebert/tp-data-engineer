# Introduction

The purpose of this interview exercise is to assess your ability to create a data pipeline from scratch.

Your goal is to retrieve some datasets, process them, then saving output into different type of storages.

We provide in this exercise a docker-compose which will allow you to deploy different services :
- mongodb instance (noSQL database)
- minio instance (storage service compatible with S3)
- airflow instance (tool for data engineering pipelines - user : airflow ; password : airflowpassword)
- postgres instance (used uniquely for airflow)

There is no need to customize docker-compose.yml, thus you will be able to focus on the data pipeline itself. We propose in this exercise Airflow for making your data engineering pipeline, because it is the tool that we use internally, but you are free to use any tool you want (open source please) if you prefer another one.

The result code should be written on Python language. Upon completion, please upload your code on a public git repository and add a README that could help us to test your code (don't fork this one for delinking state and results).

After the exercise is completed, we will take the time to discuss what has been done. There's not a single way to do things right, and we're aware of that. Please code what you feel would be naturally elegant and simple for you, not what you think we might expect.

If you're stuck on something, please reach out to us.

## Installation

First, you need to prepare folders and env file in order to deploy your services. Clone this repo, go into it, then :

```
./prepareServices.sh
```

Deploy services :

```
docker-compose up --build -d
```

# Exercise

Etalab team is exporting regularly (every week) the complete list of datasets or organizations hosted in data.gouv.fr. Data is available here: https://www.data.gouv.fr/fr/datasets/catalogue-des-donnees-de-data-gouv-fr/

From these data, you will build a data pipeline which will is scheduled every wednesday at 5PM and will perform:
1) Recuperation of the list of every dataset in data.gouv.fr
2) Filter this list and keep only those for which the term "mobilit√©" or "transport" is present
3) Export this list on csv format into a minio bucket called 'datagouv' and in a folder named with actual date (YYYY-MM-DD)
4) Filter this list by the popularity of a data producer organization keeping only datasets from TOP 30 organization (you will need the organization list available in https://www.data.gouv.fr/fr/datasets/catalogue-des-donnees-de-data-gouv-fr/ and use the column 'metric.followers' in this csv)
5) Keep only most relevant fields (dataset_id, dataset title, dataset followers, organization, organization_id, organization followers).
6) Export this list into a mongo document in a database called 'datagouv' and a collection called 'tops'

If you're stuck on something, please reach out to us.